MNIST Digit Classifier
This project consists of a convolutional neural network (CNN) designed to classify handwritten digits from the MNIST dataset. Additionally, a graphical user interface (GUI) application allows users to draw a digit on a canvas and predict the digit using the trained model.

Features
Load and preprocess the MNIST dataset.
Train a CNN model to classify handwritten digits.
Evaluate and save the trained model.
GUI application for digit prediction using the trained model.
Prerequisites
Ensure you have the following installed:

Python 3.8 or higher
NumPy
Pandas
TensorFlow
Scikit-Learn
PIL (Pillow)
tkinter
You can install the necessary Python packages using pip by running the command: pip install numpy pandas tensorflow scikit-learn pillow.

Dataset
The MNIST dataset files should be placed in the directory structure as follows:

Your MNIST dataset files, mnist_train.csv and mnist_test.csv, should be placed under a directory path like C:\Users\YourUsername\Downloads\mnist\. Replace YourUsername with your actual username on your computer.
Usage
Training the Model
To train the model, execute the part of the script that includes data loading, preprocessing, model training, and evaluation. Adjust the file paths to where you've stored the MNIST dataset on your system.

Running the GUI Application
To start the GUI application for digit prediction, run the tkinter part of the script. This will allow you to draw a digit on a canvas and click "Predict" to see the model's prediction of your drawing.

Project Structure
The project should have the following files:

mnist_train.csv: The training data file.
mnist_test.csv: The test data file.
mnist_cnn_model.h5: The file where the trained model is saved.
Model Architecture
The model architecture includes:

Three convolutional layers with ReLU activations, the first layer with 32 filters of size 3x3, followed by two layers with 64 filters of the same size.
Two max-pooling layers after the first and second convolutional layers, both with a pool size of 2x2.
A flattening layer to reshape the output for the dense layer.
Two dense layers, one with 64 units and ReLU activation and another with 10 units and softmax activation to classify the digits.
License
Please specify the license under which the project is released.
